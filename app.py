import streamlit as st
from openai import OpenAI
from pypdf import PdfReader # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è PDF
import chromadb 
import os
from chromadb.config import Settings
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer 
import sqlite3 
import datetime

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="RAG + DataBase", page_icon="üìÑ")
st.title("–ß–∞—Ç —Å –ü–∞–º—è—Ç—å—é (SQLite + LLM)")

load_dotenv() 
api_key = os.getenv("OPENROUTER_API_KEY")


if not api_key:
    st.error("–ù–µ –Ω–∞–π–¥–µ–Ω –∫–ª—é—á API! –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env –∏ –≤–ø–∏—à–∏—Ç–µ —Ç—É–¥–∞ OPENROUTER_API_KEY")
    st.stop()

# OpenRouter
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=api_key,
)

@st.cache_resource# –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –µ–¥–∏–Ω–æ—Ä–∞–∑–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ MiniLM
def load_embedding_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

embedding_model = load_embedding_model()


chroma_client = chromadb.PersistentClient(path="my_vector_db")
collection = chroma_client.get_or_create_collection(
    name="my_documents",
    metadata={"hnsw:space": "cosine"} 
)

DB_FOLDER = "data"
DB_FILE = os.path.join(DB_FOLDER, "chat_history.db")

os.makedirs(DB_FOLDER, exist_ok=True)

conn = sqlite3.connect(DB_FILE, check_same_thread=False)
cursor = conn.cursor()


# Init table
def init_db():
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT,
            role TEXT,
            content TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()

def save_message_to_db(session_id, role, content):
    cursor.execute(
        'INSERT INTO messages (session_id, role, content) VALUES (?, ?, ?)',
        (session_id, role, content)
    )
    conn.commit()

    
def load_history_from_db(session_id, limit=20):
    cursor.execute(
        'SELECT role, content FROM messages WHERE session_id = ? ORDER BY timestamp DESC LIMIT ?',
        (session_id, limit)
    )
    rows = cursor.fetchall()
    return [{"role": row[0], "content": row[1]} for row in rows][::-1]

# start table creation at startup
init_db() 


# Rag Functions
def get_pdf_text(uploaded_file):
    text = ""
    try:
        pdf_reader = PdfReader(uploaded_file)
        # –ß–∏—Ç–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        for page in pdf_reader.pages:
            text += page.extract_text()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
    return text


def split_text(text, chunk_size=500, overlap=100):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        if len(chunk) > 50: # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–æ–≤—Å–µ–º –º–µ–ª–∫–∏–µ –∫—É—Å–æ—á–∫–∏
            chunks.append(chunk)
    return chunks

def get_embedding(text):
    response = client.embeddings.create(
        model="all-minilm", 
        input=text
    )
    return response.data[0].embedding


def get_embedding(text):
    return embedding_model.encode(text).tolist()

def get_existing_files():
    data = collection.get(include=['metadatas'])
    
    unique_files = set([item['source'] for item in data['metadatas']])
    return list(unique_files)


# Interface
CURRENT_SESSION_ID = "user_default"
if "messages" not in st.session_state:
    db_history = load_history_from_db(CURRENT_SESSION_ID)
    st.session_state.messages = db_history

    if not st.session_state.messages:
        st.session_state.messages = []
        welcome_msg = "–ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π RAG-–ø–æ–º–æ—â–Ω–∏–∫. –ó–∞–≥—Ä—É–∑–∏ PDF –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –∑–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å."
        st.session_state.messages.append({"role": "assistant", "content": welcome_msg})



with st.sidebar:
    st.header("–ó–∞–≥—Ä—É–∑–∫–∞")
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ PDF —Ñ–∞–π–ª", type="pdf")
    
    if uploaded_file:
        filename = uploaded_file.name
        existing_docs = collection.get(where={"source": filename})
        
        if len(existing_docs['ids']) > 0:
            st.success(f"–§–∞–π–ª '{filename}' —É–∂–µ –µ—Å—Ç—å –≤ –±–∞–∑–µ.")
        else:
            with st.spinner("–ò–Ω–¥–µ–∫—Å–∏—Ä—É—é –Ω–æ–≤—ã–π —Ñ–∞–π–ª..."):
                text = get_pdf_text(uploaded_file)
                chunks = split_text(text)
                

                ids = []       
                metadatas = [] 
                vectors = []   
                documents_text = [] 
                
                progress = st.progress(0)
                for i, chunk in enumerate(chunks):
                    vec = get_embedding(chunk)
                    
                    ids.append(f"{filename}_chunk{i}")
                    metadatas.append({"source": filename})
                    vectors.append(vec)
                    documents_text.append(chunk)
                    
                    progress.progress((i+1)/len(chunks))

                metadatas = [{"source": filename} for _ in chunks]

                collection.add(
                    ids=ids,
                    embeddings=vectors,
                    documents=documents_text, 
                    metadatas=metadatas
                )
                st.success("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –±–∞–∑—É.")

    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞"):
        cursor.execute("DELETE FROM messages WHERE session_id = ?", (CURRENT_SESSION_ID,))
        conn.commit()
        st.session_state.messages = []
        st.rerun() # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É

    st.divider()
    files_list = get_existing_files()
    options = ["–í–æ –≤—Å–µ–π –±–∞–∑–µ"] + files_list
    selected_file = st.selectbox("–ì–¥–µ –∏—Å–∫–∞—Ç—å –æ—Ç–≤–µ—Ç?", options)

# Chat drawing
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("–í–æ–ø—Ä–æ—Å..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    save_message_to_db(CURRENT_SESSION_ID, "user", prompt)
    
    query_vec = get_embedding(prompt)
    search_params = {
        "query_embeddings": [query_vec],
        "n_results": 10
    }
    
    if selected_file != "–í–æ –≤—Å–µ–π –±–∞–∑–µ":
        search_params["where"] = {"source": selected_file}

    results = collection.query(**search_params)
    valid_chunks = []

    # Information found in the database
    with st.expander("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–ß—Ç–æ –Ω–∞—à–ª–∞ –±–∞–∑–∞)"):
        found_chunks = results['documents'][0]
        distances = results['distances'][0]
            
        for i, dist in enumerate(distances):
            chunk_text = found_chunks[i]
            st.write(f"**–ö—É—Å–æ–∫ {i+1}** (–î–∏—Å—Ç–∞–Ω—Ü–∏—è: {dist:.4f}):")
            st.caption(chunk_text[:200] + "...") # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ –∫—É—Å–∫–∞
                
            # –§–∏–ª—å—Ç—Ä: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–∏—Å—Ç–∞–Ω—Ü–∏—è –º–µ–Ω—å—à–µ 0.7 (–º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å)
            if dist < 0.7:
                st.success("–ü–æ–¥—Ö–æ–¥–∏—Ç")
                valid_chunks.append(chunk_text)
            else:
                st.warning("–≠—Ç–æ—Ç –∫—É—Å–æ–∫ –æ—Ç–±—Ä–æ—à–µ–Ω (—Å–ª–∏—à–∫–æ–º –Ω–µ–ø–æ—Ö–æ–∂)")

 
    if not valid_chunks:
        system_prompt = "–¢—ã —É–º–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç."
    else:
        context_text = "\n---\n".join(valid_chunks)
        system_prompt = f"""
        –¢—ã ‚Äî —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–≥—Ä—É–∑–∏–ª –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∏ –Ω–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–æ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ.
        –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ.
        –ù–µ –≥–æ–≤–æ—Ä–∏ "—è –Ω–µ –≤–∏–∂—É —Ñ–∞–π–ª–æ–≤" –∏–ª–∏ "–≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ". –û—Ç–≤–µ—á–∞–π —Ç–∞–∫, –±—É–¥—Ç–æ —Ç—ã –ø—Ä–æ—á–∏—Ç–∞–ª —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Ü–µ–ª–∏–∫–æ–º.

        –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n{context_text}
        """

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (OpenRouter)
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            stream = client.chat.completions.create(
                model="meta-llama/llama-3.3-70b-instruct:free", # –ò–ª–∏ "google/gemma-2-9b-it:free"
                messages=[
                    {"role": "system", "content": system_prompt},
                    *[{"role": m["role"], "content": m["content"]} for m in st.session_state.messages[-10:]]
                ],
                stream=True,
                extra_headers={
                    "HTTP-Referer": "http://localhost:8501",
                    "X-Title": "Local RAG App"
                }
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content
                    message_placeholder.markdown(full_response + "‚ñå") # ‚ñå - —ç—Ç–æ –∫—É—Ä—Å–æ—Ä
            message_placeholder.markdown(full_response) # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –∫—É—Ä—Å–æ—Ä–∞
            st.session_state.messages.append({"role": "assistant", "content": full_response})

            # Save in Database
            save_message_to_db(CURRENT_SESSION_ID, "assistant", full_response)

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ API: {e}")
        
